{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abb7d81-83b4-447a-ae80-902ee7ac222b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fab3a84-0103-45f5-93ef-7a080a647e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\"\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(\"wine_dataset.csv\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "print(\"Dataset downloaded and saved as 'wine_dataset.csv'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ddaf69-90b6-4fbd-9ee8-6ca4a2535ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "column_names = [\n",
    "    \"Class\",\n",
    "    \"Alcohol\",\n",
    "    \"Malic Acid\",\n",
    "    \"Ash\",\n",
    "    \"Alcalinity of Ash\",\n",
    "    \"Magnesium\",\n",
    "    \"Total Phenols\",\n",
    "    \"Flavanoids\",\n",
    "    \"Nonflavanoid Phenols\",\n",
    "    \"Proanthocyanins\",\n",
    "    \"Color Intensity\",\n",
    "    \"Hue\",\n",
    "    \"OD280/OD315 of Diluted Wines\",\n",
    "    \"Proline\"\n",
    "]\n",
    "\n",
    "df = pd.read_csv(\"wine_dataset.csv\", header=None, names=column_names)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b85cef2-ad49-4d3a-a403-498684bf39c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define column names for the dataset\n",
    "column_names = [\n",
    "    \"Class\",\n",
    "    \"Alcohol\",\n",
    "    \"Malic Acid\",\n",
    "    \"Ash\",\n",
    "    \"Alcalinity of Ash\",\n",
    "    \"Magnesium\",\n",
    "    \"Total Phenols\",\n",
    "    \"Flavanoids\",\n",
    "    \"Nonflavanoid Phenols\",\n",
    "    \"Proanthocyanins\",\n",
    "    \"Color Intensity\",\n",
    "    \"Hue\",\n",
    "    \"OD280/OD315 of Diluted Wines\",\n",
    "    \"Proline\"\n",
    "]\n",
    "\n",
    "# Load the dataset into a Pandas DataFrame\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\"\n",
    "df = pd.read_csv(url, header=None, names=column_names)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722af469-af9e-4e80-ba8e-283048ebff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define column names for the dataset\n",
    "column_names = [\n",
    "    \"Class\",\n",
    "    \"Alcohol\",\n",
    "    \"Malic Acid\",\n",
    "    \"Ash\",\n",
    "    \"Alcalinity of Ash\",\n",
    "    \"Magnesium\",\n",
    "    \"Total Phenols\",\n",
    "    \"Flavanoids\",\n",
    "    \"Nonflavanoid Phenols\",\n",
    "    \"Proanthocyanins\",\n",
    "    \"Color Intensity\",\n",
    "    \"Hue\",\n",
    "    \"OD280/OD315 of Diluted Wines\",\n",
    "    \"Proline\"\n",
    "]\n",
    "\n",
    "# Load the dataset into a Pandas DataFrame\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\"\n",
    "df = pd.read_csv(url, header=None, names=column_names)\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = df.drop(\"Class\", axis=1)  # Features\n",
    "y = df[\"Class\"]  # Target variable\n",
    "\n",
    "# Display the first few rows of the features and target variables\n",
    "print(\"Features:\")\n",
    "print(X.head())\n",
    "\n",
    "print(\"\\nTarget Variable:\")\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52fd0c7-b5a9-4eff-bece-0e93bfa20558",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the features\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Display the first few rows of the scaled features\n",
    "print(\"Scaled Features:\")\n",
    "print(pd.DataFrame(X_scaled, columns=X.columns).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b86941-9c46-4569-9624-9a6cfdc11b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create an imputer instance\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "\n",
    "# Impute missing values (this step is not necessary for the Wine dataset)\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Display the first few rows of the imputed features\n",
    "print(\"Imputed Features:\")\n",
    "print(pd.DataFrame(X_imputed, columns=X.columns).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd4b429-48bc-447a-871c-e96f1181de05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "# Preprocessed dataset (scaled or imputed)\n",
    "X_preprocessed = X_scaled  # Use X_scaled or X_imputed depending on your preprocessing choice\n",
    "\n",
    "# Initialize PCA with the desired number of components\n",
    "num_components = 2  # Choose the number of components you want\n",
    "pca = PCA(n_components=num_components)\n",
    "\n",
    "# Fit PCA to the preprocessed data and transform the data\n",
    "X_pca = pca.fit_transform(X_preprocessed)\n",
    "\n",
    "# Create a DataFrame for the transformed data with PCA components\n",
    "pca_column_names = [f\"PC{i+1}\" for i in range(num_components)]\n",
    "X_pca_df = pd.DataFrame(X_pca, columns=pca_column_names)\n",
    "\n",
    "# Display the first few rows of the PCA-transformed data\n",
    "print(\"PCA Transformed Features:\")\n",
    "print(X_pca_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ff1d3f-9e94-42b8-8a34-973fb380d23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Preprocessed dataset (scaled or imputed)\n",
    "X_preprocessed = X_scaled  # Use X_scaled or X_imputed depending on your preprocessing choice\n",
    "\n",
    "# Initialize PCA\n",
    "pca = PCA()\n",
    "\n",
    "# Fit PCA to the preprocessed data\n",
    "pca.fit(X_preprocessed)\n",
    "\n",
    "# Calculate cumulative explained variance ratio\n",
    "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Plot the cumulative explained variance ratio\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.title('Cumulative Explained Variance Ratio vs. Number of Principal Components')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e71cdd-e178-4195-9073-9c8a0572cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Preprocessed dataset (scaled or imputed)\n",
    "X_preprocessed = X_scaled  # Use X_scaled or X_imputed depending on your preprocessing choice\n",
    "\n",
    "# Initialize PCA with the desired number of components\n",
    "num_components = 2  # Choose the number of components you want\n",
    "pca = PCA(n_components=num_components)\n",
    "\n",
    "# Fit PCA to the preprocessed data and transform the data\n",
    "X_pca = pca.fit_transform(X_preprocessed)\n",
    "\n",
    "# Create a DataFrame for the transformed data with PCA components\n",
    "pca_column_names = [f\"PC{i+1}\" for i in range(num_components)]\n",
    "X_pca_df = pd.DataFrame(X_pca, columns=pca_column_names)\n",
    "\n",
    "# Add the \"Class\" column from the original dataset to the PCA-transformed DataFrame\n",
    "X_pca_df[\"Class\"] = y\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x=\"PC1\", y=\"PC2\", hue=\"Class\", data=X_pca_df, palette=\"viridis\")\n",
    "plt.title(\"PCA Scatter Plot\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82e767b-1a95-4ba2-9920-e08d27497667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessed dataset (scaled or imputed)\n",
    "X_preprocessed = X_scaled  # Use X_scaled or X_imputed depending on your preprocessing choice\n",
    "\n",
    "# Initialize PCA with the desired number of components\n",
    "num_components = 2  # Choose the number of components you want\n",
    "pca = PCA(n_components=num_components)\n",
    "\n",
    "# Fit PCA to the preprocessed data and transform the data\n",
    "X_pca = pca.fit_transform(X_preprocessed)\n",
    "\n",
    "# Perform K-Means clustering on the PCA-transformed data\n",
    "num_clusters = 3  # Choose the number of clusters\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X_pca)\n",
    "\n",
    "# Add the cluster labels to the PCA-transformed DataFrame\n",
    "X_pca_df = pd.DataFrame(X_pca, columns=[f\"PC{i+1}\" for i in range(num_components)])\n",
    "X_pca_df[\"Cluster\"] = cluster_labels\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x=\"PC1\", y=\"PC2\", hue=\"Cluster\", data=X_pca_df, palette=\"Set1\", s=100)\n",
    "plt.title(\"K-Means Clustering on PCA-Transformed Data\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920682ae-2ebb-46c7-b090-4c0ef84c7f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sure, let's interpret the results of the PCA and clustering analysis:\n",
    "\n",
    "PCA Results:\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique that aims\n",
    "to capture the most important information in the original data by projecting it onto\n",
    "a lower-dimensional space. In the context of the Wine dataset, you applied PCA to reduce \n",
    "the original features into a two-dimensional space. The two principal components represent\n",
    "linear combinations of the original features that maximize the explained variance.\n",
    "\n",
    "PCA Components: The first principal component (PC1) captures the direction of maximum variance\n",
    "in the data, while the second principal component (PC2) captures the second highest variance \n",
    "orthogonal to the first component. Each principal component is a linear combination of the original features.\n",
    "\n",
    "Explained Variance Ratio: You may observe that the cumulative explained variance ratio plot \n",
    "\n",
    "\n",
    "\n",
    "shows a curve that flattens out after the first few components. The \"elbow point\" on the plot \n",
    "suggests a point of diminishing returns where adding more components doesn't contribute significantly\n",
    "to the explained variance. This can guide you in choosing an appropriate number of components for dimensionality reduction.\n",
    "\n",
    "Clustering Results (K-Means):\n",
    "Clustering is an unsupervised learning technique that groups similar data points together based\n",
    "on a certain similarity measure. In this case, you applied K-Means clustering to the PCA-transformed\n",
    "data to group similar wine samples together.\n",
    "\n",
    "Number of Clusters: You chose a specific number of clusters (e.g., 3 clusters) for the K-Means algorithm. \n",
    "Each cluster represents a group of data points that are similar to each other based on their PCA-transformed features.\n",
    "\n",
    "Cluster Interpretation: By examining the scatter plot of the clustered data points, you can observe how the \n",
    "K-Means algorithm has partitioned the data into distinct groups in the reduced-dimensional space.\n",
    "Each cluster is represented by a different color on the plot. The separation of clusters implies\n",
    "that the K-Means algorithm has found distinct patterns or groups in the data based on the PCA-transformed features.\n",
    "\n",
    "Remember that interpretation might depend on domain knowledge. In a real-world scenario, you would \n",
    "often try to assign meaning to the clusters based on your understanding of the data and the problem \n",
    "you're trying to solve. Visualizations and statistical measures can help provide insights, but ultimately,\n",
    "the interpretation should be informed by the context of the data and the specific goals of your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e37ac35-966d-4dce-a616-0a4bffb4cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "..........................The End.........................."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
